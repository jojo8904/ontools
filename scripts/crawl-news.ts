#!/usr/bin/env tsx
/**
 * News Crawler Script
 * Fetches RSS feeds, summarizes with Claude Haiku, and saves to Supabase
 */

import Parser from 'rss-parser'
import Anthropic from '@anthropic-ai/sdk'
import { createClient } from '@supabase/supabase-js'

// Environment variables
const SUPABASE_URL = process.env.SUPABASE_URL!
const SUPABASE_SERVICE_ROLE_KEY = process.env.SUPABASE_SERVICE_ROLE_KEY!
const CLAUDE_API_KEY = process.env.CLAUDE_API_KEY!

// Supabase client (service_role for server-side writes)
const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)

// RSS sources
const RSS_SOURCES = [
  // IT/Tech
  { url: 'https://www.etnews.com/rss.xml', category: 'tech', source: 'ì „ìì‹ ë¬¸' },
  { url: 'https://www.bloter.net/feed', category: 'tech', source: 'ë¸”ë¡œí„°' },
  { url: 'https://www.boannews.com/media/rss.xml', category: 'tech', source: 'ë³´ì•ˆë‰´ìŠ¤' },

  // Finance
  { url: 'https://www.hankyung.com/feed/economy', category: 'finance', source: 'í•œêµ­ê²½ì œ' },
  { url: 'https://www.mk.co.kr/rss/30100041/', category: 'finance', source: 'ë§¤ì¼ê²½ì œ' },

  // Labor
  { url: 'https://www.moel.go.kr/rss/policy.do', category: 'labor', source: 'ê³ ìš©ë…¸ë™ë¶€' },

  // Health
  { url: 'https://news.google.com/rss/search?q=%EA%B1%B4%EA%B0%95+%EC%9D%98%EB%A3%8C+%EB%B3%B4%EA%B1%B4&hl=ko&gl=KR&ceid=KR:ko', category: 'health', source: 'Googleë‰´ìŠ¤ ê±´ê°•' },

  // Energy
  { url: 'https://news.google.com/rss/search?q=%EC%97%90%EB%84%88%EC%A7%80+%EC%A0%84%EA%B8%B0%EC%9A%94%EA%B8%88+%EC%A0%84%EB%A0%A5&hl=ko&gl=KR&ceid=KR:ko', category: 'energy', source: 'Googleë‰´ìŠ¤ ì—ë„ˆì§€' },
]

// Tool mapping keywords
const TOOL_KEYWORDS: Record<string, string[]> = {
  salary: ['ìµœì €ì„ê¸ˆ', 'ì—°ë´‰', 'ê¸‰ì—¬', 'ì„ê¸ˆ', 'ì›”ê¸‰', 'ì‹¤ìˆ˜ë ¹ì•¡'],
  retirement: ['í‡´ì§ê¸ˆ', 'í‡´ì§', 'ê·¼ì†'],
  currency: ['í™˜ìœ¨', 'ë‹¬ëŸ¬', 'ì—”í™”', 'ìœ„ì•ˆ', 'ìœ ë¡œ', 'ì›í™”'],
  bmi: ['ë¹„ë§Œ', 'BMI', 'ì²´ì¤‘', 'ê±´ê°•', 'ë‹¤ì´ì–´íŠ¸'],
  electricity: ['ì „ê¸°ìš”ê¸ˆ', 'ì „ê¸°ì„¸', 'ëˆ„ì§„ì œ', 'ì „ë ¥'],
  dday: ['ê³µíœ´ì¼', 'íœ´ì¼', 'ì—°íœ´', 'ëª…ì ˆ'],
  unit: ['ë‹¨ìœ„', 'ë³€í™˜'],
}

// Claude API client
const claude = new Anthropic({ apiKey: CLAUDE_API_KEY })

// Check if news already exists
async function newsExists(url: string): Promise<boolean> {
  const { data } = await supabase
    .from('news')
    .select('id')
    .eq('url', url)
    .limit(1)
  return (data?.length ?? 0) > 0
}

// Summarize news with Claude Haiku
async function summarizeNews(title: string, content: string): Promise<{
  summary: string
  categories: string[]
  related_tools: string[]
}> {
  const prompt = `ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ì œëª©: ${title}
ë‚´ìš©: ${content}

ì‘ë‹µ í˜•ì‹:
{
  "summary": "200ì ë‚´ì™¸ì˜ í•œêµ­ì–´ ìš”ì•½",
  "categories": ["ì¹´í…Œê³ ë¦¬ ë°°ì—´ (tech, finance, labor, health, energy, general ì¤‘ ì„ íƒ)"],
  "related_tools": ["ê´€ë ¨ ë„êµ¬ ë°°ì—´ (salary, currency, retirement, bmi, electricity, dday, unit ì¤‘ ì„ íƒ)"]
}

ê·œì¹™:
1. summaryëŠ” í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ 200ì ë‚´ì™¸ë¡œ ì‘ì„±
2. categoriesëŠ” 1-2ê°œ ì„ íƒ (ì£¼ìš” ì¹´í…Œê³ ë¦¬ë§Œ)
3. related_toolsëŠ” ë‰´ìŠ¤ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ë„êµ¬ë§Œ í¬í•¨ (ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
4. JSONë§Œ ë°˜í™˜ (ì„¤ëª… ì—†ì´)`

  const message = await claude.messages.create({
    model: 'claude-haiku-4-5-20251001',
    max_tokens: 1024,
    messages: [{ role: 'user', content: prompt }],
  })

  const responseText = message.content[0].type === 'text' ? message.content[0].text : '{}'
  const jsonMatch = responseText.match(/\{[\s\S]*\}/)

  if (!jsonMatch) {
    throw new Error('Failed to parse Claude response')
  }

  return JSON.parse(jsonMatch[0])
}

// Save news to Supabase
async function saveNews(newsItem: {
  title: string
  summary: string
  original_content: string
  source: string
  published_at: string
  categories: string[]
  related_tools: string[]
  url: string
}) {
  const { error } = await supabase.from('news').insert(newsItem)
  if (error) throw new Error(`Supabase insert error: ${error.message}`)
}

// Main crawler function
async function crawlNews() {
  const parser = new Parser()
  let totalProcessed = 0
  let totalSaved = 0
  let totalSkipped = 0
  let totalErrors = 0

  console.log('ğŸš€ Starting news crawler...')
  console.log(`ğŸ“¡ Processing ${RSS_SOURCES.length} RSS sources`)

  for (const { url, category, source } of RSS_SOURCES) {
    console.log(`\nğŸ“° Fetching from ${source} (${category})...`)

    try {
      const feed = await parser.parseURL(url)
      console.log(`  Found ${feed.items.length} items`)

      // Process latest 10 items per source
      for (const item of feed.items.slice(0, 10)) {
        totalProcessed++

        if (!item.link || !item.title) {
          console.log(`  â­ï¸  Skipping item without link or title`)
          totalSkipped++
          continue
        }

        // Check if already exists
        const exists = await newsExists(item.link)
        if (exists) {
          console.log(`  â­ï¸  Already exists: ${item.title.substring(0, 50)}...`)
          totalSkipped++
          continue
        }

        try {
          // Summarize with Claude
          console.log(`  ğŸ¤– Summarizing: ${item.title.substring(0, 50)}...`)
          const { summary, categories, related_tools } = await summarizeNews(
            item.title,
            item.contentSnippet || item.content || item.title
          )

          // Save to Supabase
          await saveNews({
            title: item.title,
            summary,
            original_content: item.contentSnippet || item.content || item.title,
            source,
            published_at: item.pubDate ? new Date(item.pubDate).toISOString() : new Date().toISOString(),
            categories: [...new Set([category, ...categories])],
            related_tools,
            url: item.link,
          })

          console.log(`  âœ… Saved: ${item.title.substring(0, 50)}...`)
          totalSaved++

          // Rate limiting for Claude API
          await new Promise(resolve => setTimeout(resolve, 1000))
        } catch (error) {
          console.error(`  âŒ Error processing item: ${error}`)
          totalErrors++
        }
      }
    } catch (error) {
      console.error(`  âŒ Error fetching ${source}: ${error}`)
      totalErrors++
    }
  }

  console.log(`\nğŸ“Š Crawler Summary:`)
  console.log(`  Total processed: ${totalProcessed}`)
  console.log(`  Saved: ${totalSaved}`)
  console.log(`  Skipped (duplicates): ${totalSkipped}`)
  console.log(`  Errors: ${totalErrors}`)
}

// Run crawler
crawlNews().catch(error => {
  console.error('Fatal error:', error)
  process.exit(1)
})
