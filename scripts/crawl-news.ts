#!/usr/bin/env tsx
/**
 * News Crawler Script
 * Fetches RSS feeds, summarizes with Claude Haiku, and saves to bkend.ai
 */

import Parser from 'rss-parser'
import Anthropic from '@anthropic-ai/sdk'

// Environment variables
const BKEND_API_URL = process.env.BKEND_API_URL || 'https://api-client.bkend.ai/v1'
const BKEND_PROJECT_ID = process.env.BKEND_PROJECT_ID!
const BKEND_ENV = process.env.BKEND_ENV || 'production'
const CLAUDE_API_KEY = process.env.CLAUDE_API_KEY!

// RSS sources from Plan document
const RSS_SOURCES = [
  // IT/Tech
  { url: 'https://it.chosun.com/site/data/rss/rss.xml', category: 'tech', source: 'ITì¡°ì„ ' },
  { url: 'https://www.etnews.com/rss.xml', category: 'tech', source: 'ì „ìì‹ ë¬¸' },
  { url: 'https://zdnet.co.kr/rss/news.xml', category: 'tech', source: 'ZDNet Korea' },

  // Finance
  { url: 'https://www.hankyung.com/feed/economy', category: 'finance', source: 'í•œêµ­ê²½ì œ' },
  { url: 'https://www.mk.co.kr/rss/30100041/', category: 'finance', source: 'ë§¤ì¼ê²½ì œ' },

  // Labor
  { url: 'https://www.moel.go.kr/rss/news.xml', category: 'labor', source: 'ê³ ìš©ë…¸ë™ë¶€' },

  // Health
  { url: 'https://health.chosun.com/rss/health.xml', category: 'health', source: 'í—¬ìŠ¤ì¡°ì„ ' },

  // Energy
  { url: 'https://www.ekn.kr/rss/economy.xml', category: 'energy', source: 'ì—ë„ˆì§€ê²½ì œ' },
]

// Tool mapping keywords
const TOOL_KEYWORDS: Record<string, string[]> = {
  salary: ['ìµœì €ì„ê¸ˆ', 'ì—°ë´‰', 'ê¸‰ì—¬', 'ì„ê¸ˆ', 'ì›”ê¸‰', 'ì‹¤ìˆ˜ë ¹ì•¡'],
  retirement: ['í‡´ì§ê¸ˆ', 'í‡´ì§', 'ê·¼ì†'],
  currency: ['í™˜ìœ¨', 'ë‹¬ëŸ¬', 'ì—”í™”', 'ìœ„ì•ˆ', 'ìœ ë¡œ', 'ì›í™”'],
  bmi: ['ë¹„ë§Œ', 'BMI', 'ì²´ì¤‘', 'ê±´ê°•', 'ë‹¤ì´ì–´íŠ¸'],
  electricity: ['ì „ê¸°ìš”ê¸ˆ', 'ì „ê¸°ì„¸', 'ëˆ„ì§„ì œ', 'ì „ë ¥'],
  dday: ['ê³µíœ´ì¼', 'íœ´ì¼', 'ì—°íœ´', 'ëª…ì ˆ'],
  unit: ['ë‹¨ìœ„', 'ë³€í™˜'],
}

// bkend.ai API client
async function bkendFetch(path: string, options: RequestInit = {}) {
  const res = await fetch(`${BKEND_API_URL}${path}`, {
    ...options,
    headers: {
      'Content-Type': 'application/json',
      'x-project-id': BKEND_PROJECT_ID,
      'x-environment': BKEND_ENV,
      ...options.headers,
    },
  })

  if (!res.ok) {
    const errorText = await res.text()
    throw new Error(`bkend.ai API error: ${res.status} ${errorText}`)
  }

  return res.json()
}

// Claude API client
const claude = new Anthropic({ apiKey: CLAUDE_API_KEY })

// Check if news already exists
async function newsExists(url: string): Promise<boolean> {
  try {
    const response = await bkendFetch(`/data/news?filter=${encodeURIComponent(JSON.stringify({ url }))}`)
    return response.data && response.data.length > 0
  } catch {
    return false
  }
}

// Summarize news with Claude Haiku
async function summarizeNews(title: string, content: string): Promise<{
  summary: string
  categories: string[]
  related_tools: string[]
}> {
  const prompt = `ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ì œëª©: ${title}
ë‚´ìš©: ${content}

ì‘ë‹µ í˜•ì‹:
{
  "summary": "200ì ë‚´ì™¸ì˜ í•œêµ­ì–´ ìš”ì•½",
  "categories": ["ì¹´í…Œê³ ë¦¬ ë°°ì—´ (tech, finance, labor, health, energy, general ì¤‘ ì„ íƒ)"],
  "related_tools": ["ê´€ë ¨ ë„êµ¬ ë°°ì—´ (salary, currency, retirement, bmi, electricity, dday, unit ì¤‘ ì„ íƒ)"]
}

ê·œì¹™:
1. summaryëŠ” í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ 200ì ë‚´ì™¸ë¡œ ì‘ì„±
2. categoriesëŠ” 1-2ê°œ ì„ íƒ (ì£¼ìš” ì¹´í…Œê³ ë¦¬ë§Œ)
3. related_toolsëŠ” ë‰´ìŠ¤ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ë„êµ¬ë§Œ í¬í•¨ (ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
4. JSONë§Œ ë°˜í™˜ (ì„¤ëª… ì—†ì´)`

  const message = await claude.messages.create({
    model: 'claude-haiku-4-20250514', // Haiku 4.5
    max_tokens: 1024,
    messages: [{ role: 'user', content: prompt }],
  })

  const responseText = message.content[0].type === 'text' ? message.content[0].text : '{}'
  const jsonMatch = responseText.match(/\{[\s\S]*\}/)

  if (!jsonMatch) {
    throw new Error('Failed to parse Claude response')
  }

  return JSON.parse(jsonMatch[0])
}

// Save news to bkend.ai
async function saveNews(newsItem: {
  title: string
  summary: string
  original_content: string
  source: string
  published_at: string
  categories: string[]
  related_tools: string[]
  url: string
}) {
  return bkendFetch('/data/news', {
    method: 'POST',
    body: JSON.stringify(newsItem),
  })
}

// Main crawler function
async function crawlNews() {
  const parser = new Parser()
  let totalProcessed = 0
  let totalSaved = 0
  let totalSkipped = 0
  let totalErrors = 0

  console.log('ğŸš€ Starting news crawler...')
  console.log(`ğŸ“¡ Processing ${RSS_SOURCES.length} RSS sources`)

  for (const { url, category, source } of RSS_SOURCES) {
    console.log(`\nğŸ“° Fetching from ${source} (${category})...`)

    try {
      const feed = await parser.parseURL(url)
      console.log(`  Found ${feed.items.length} items`)

      // Process latest 10 items per source
      for (const item of feed.items.slice(0, 10)) {
        totalProcessed++

        if (!item.link || !item.title) {
          console.log(`  â­ï¸  Skipping item without link or title`)
          totalSkipped++
          continue
        }

        // Check if already exists
        const exists = await newsExists(item.link)
        if (exists) {
          console.log(`  â­ï¸  Already exists: ${item.title.substring(0, 50)}...`)
          totalSkipped++
          continue
        }

        try {
          // Summarize with Claude
          console.log(`  ğŸ¤– Summarizing: ${item.title.substring(0, 50)}...`)
          const { summary, categories, related_tools } = await summarizeNews(
            item.title,
            item.contentSnippet || item.content || item.title
          )

          // Save to bkend.ai
          await saveNews({
            title: item.title,
            summary,
            original_content: item.contentSnippet || item.content || item.title,
            source,
            published_at: item.pubDate ? new Date(item.pubDate).toISOString() : new Date().toISOString(),
            categories: [...new Set([category, ...categories])], // Merge with source category
            related_tools,
            url: item.link,
          })

          console.log(`  âœ… Saved: ${item.title.substring(0, 50)}...`)
          totalSaved++

          // Rate limiting for Claude API
          await new Promise(resolve => setTimeout(resolve, 1000))
        } catch (error) {
          console.error(`  âŒ Error processing item: ${error}`)
          totalErrors++
        }
      }
    } catch (error) {
      console.error(`  âŒ Error fetching ${source}: ${error}`)
      totalErrors++
    }
  }

  console.log(`\nğŸ“Š Crawler Summary:`)
  console.log(`  Total processed: ${totalProcessed}`)
  console.log(`  Saved: ${totalSaved}`)
  console.log(`  Skipped (duplicates): ${totalSkipped}`)
  console.log(`  Errors: ${totalErrors}`)
}

// Run crawler
crawlNews().catch(error => {
  console.error('Fatal error:', error)
  process.exit(1)
})
